{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c421744c-40d9-4825-a546-40450e6663c5",
   "metadata": {},
   "source": [
    "# PA 3: Evaluation and Comparison on Deep learning Models\n",
    "Trina Brough Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c04ac3-264d-4989-8b77-0bfec1eaf275",
   "metadata": {},
   "source": [
    "## 1. Understanding Evaluation Metrics (20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec6b1d-e772-44c6-a94d-6305be1f7bb7",
   "metadata": {},
   "source": [
    "### 1.1 Please explain the following commonly used evaluation metrics. (10 points)\n",
    "- Accuracy: Accuracy is how often the model predicts the outcome correctly. It is the percentage correct calculated by: the number of samples correctly classified divided by the total number of samples. It answers the question: how often is the model right? Accuracy is simple to calculate and easy to understand but it is not very helpful if our classes are imbalanced or if what we really care about is predicting events that rarely occur.\n",
    "- Precision: Precision measures how often a model correctly predicts the positive class out of the predicted positives. It is the percentage calculated by: the number of true positive predictions divided by the total number of positive predictions (both true and false). It answers the question: how often are the positive predictions correct? Precision addresses the problem that accuracy has when dealing with unbalanced classes. It is especially helpful when the cost of a false positive is high (but are okay if you miss some positives (false negatives)). Precision is best when we care more about \"being right\" than detecting them all.\n",
    "- Recall: Recall measures how often a model correctly predicts the positive (true positives) out of all the actual positives. It is the percentage calculated by: the number of true positive predictions divided by the total number of positives (true positives plus false negatives). It answers the question: how well does a model find all instances of the positive class? Recall addressed the problem that accuracy has when dealing with unbalanced classes. It is especially helpful when the cost of a false negative is high (but are okay with some false positives). Recall is best when we care more about detecting them all than \"being right\".\n",
    "- F1 Score: The F1 score is a way of averaging the precision and recall rates into one value. Because precision and recall are both rates, it creates this using the harmonic mean: 2 * (Precision * Recall) / (Precision + Recall). The result is a number between 0 and 1 that indicates how well a model classifies samples into their correct classes (0 being not classifying anything correctly and 1 being classifying all samples correctly). F1 scores is used to evaluate LLM accuracy as well as binary and multi-class classification problems (especially when classes are unbalanced). It is useful when wanting to account for both precision and recall and the costs of false negatives and false positives are relatively even. If one is more costly than the other, using straight precision or recall would be best.\n",
    "- ROC Curve and AUC (Area Under the Curve): ROC is a graph that plots the True Positive Rate (Recall) on the y axis against the False Positive Rate (1 -  Precision, can be thought of as the \"False Alarm Rate\") on the x axis. The multiple curves are created for the classification model using different thresholds. These can then be compared on the ROC curve. The random baseline will be a straight line from the bottom left of the graph to the top right of the graph. The more the ROC curves upwards away from the baseline, the better the model is. (A perfect ROC curve would actually run staight up the y axis until 1, then turn right and run straight vertically). This upward curve is quantified by the AUC metric. It measures the area under the curve and allows us to determined which threshold is best (the largest AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6075e0f-70e0-4a9c-acff-073651f1b996",
   "metadata": {},
   "source": [
    "### 1.2 Provide a practical scenario and explain which metric(s) should be chosen to assess the model performance in that scenario. (10 points)\n",
    "Scenario: A binary classification model to identify tumors in mammogram images. The data used for this model would have unbalanced classes, since there are many more mammograms taken of healthy breast tissue than of tumors. (Of all women getting mammograms, only 0.5% have breast cancer). This means that accuracy would not be a good metric (we could classify as tumor-free every time and would be correct 99.5% of the time). We could choose to use the F1 score if the cost of false negatives and false positives is similar. However, in this case, a false positive means a woman would have the stress of coming in for more diagnostic testing but a false negative would prevent her from receiving possible life-saving treatment. The cost of a false negative is much higher than a false positive. In this case recall would be our most important metric. We want to make sure we detect all the true positives even if we end up with some false positives because lives are at stake. This isn't true without bounds, however. We could, in theory, classify every image as positive and we'd have 100% recall. This is the same problem as accuracy on the flip-side. This would result in a large cost (and stress) burden as women are re-tested unnecessarily. We might, therefore, want to plot this model using different thresholds on an ROC curve. We want to maximize our recall, which would induce us to perhaps over-categorize positives (perhaps adjusting our threshold more towards a positive classification). The ROC curves would allow us to compare the different thresholds while balancing our recall against the false positive rate. The AUC would quantify which threshold is best (in case it's not visually clear on the graph).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df697539-65f3-4e67-8148-cccba36bf51c",
   "metadata": {},
   "source": [
    "## 2. Model Comparison (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920e5ea-be4b-47ba-8803-43d5fc01cc49",
   "metadata": {},
   "source": [
    "### 2.1 Choose THREE commonly used deep-learning models and train these models for a specific task (e.g., image/text classification). (20 points)\n",
    "\n",
    "To gain understanding of the metrics described above, I'd like to choose three image classification models that would likely use different metrics for evaluations. To that end, I will use:\n",
    "-the MNIST as a multi-class image classification task with balanced classes. I will use the model we developed for PA 1.\n",
    "-\n",
    "\n",
    "[TODO: Describe what models/which tasks do you want to choose. Explain the motivation.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef786-3b83-46c6-8899-62bf46ce18b4",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate these models using appropriate metrics, and analyze the results. Complete this comparison using Jupyter Notebook. (40 points)\n",
    "[TODO: Present the evaluation results with appropriate figures and tables here.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "851ff360-5b74-4f54-bbc7-ef295c981d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef783cc-d897-4c85-92be-fe2fa0437c53",
   "metadata": {},
   "source": [
    "#### Model 1: MNIST Multi-Class Classification - Balanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f40de-ea1a-4ddf-8c83-0c583553ce54",
   "metadata": {},
   "source": [
    "##### Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d81f5c29-53b9-4f7b-8482-b25394fe4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 38707486.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 13621708.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4543412.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "# ref: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# train data\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test data\n",
    "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae61abc-88a5-410c-8797-bb95aedf1697",
   "metadata": {},
   "source": [
    "##### Build MLP model for MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7256b796-154e-4eea-9ed5-b381b3070dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your Multi-Layer Perceptron (MLP) model\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: build layers here\n",
    "        self.linear_RELU_stack = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(input_size, hidden_size), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: build the networks\n",
    "        x = self.linear_RELU_stack(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071498e-3c13-4361-8718-57161ba32749",
   "metadata": {},
   "source": [
    "##### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23b09ebd-2f9e-4801-8001-28380f7be44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLP model\n",
    "\n",
    "input_size = 28 * 28  # what's input size? hints: flattened size for one instance\n",
    "hidden_size = 528  # define on your own\n",
    "output_size = 10  # what's output size for this classification tasks?\n",
    "model = MLP(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b22b13-cebd-469f-9102-63fff02a8cdc",
   "metadata": {},
   "source": [
    "##### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce78f29-e9c2-4239-9168-ea46467ad57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "learning_rate =  .001 # define your own learning rate here\n",
    "criterion = torch.nn.CrossEntropyLoss()  # which loss function should we use here? Using Cross Entropy, online this is mentioned as a good loss function for MNIST categorization\n",
    "optimizer = torch.optim.SGD (model.parameters(), lr = learning_rate)  # which optimizer do you use? Using Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d543-137e-42fa-83d1-71c2a6a114c3",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca4dfa5c-5d26-4e84-85b7-d31ac22cc339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Epoch 1/15, Train Loss: 2.175731133042114\n",
      "In Epoch 2/15, Train Loss: 1.7595097409890914\n",
      "In Epoch 3/15, Train Loss: 1.1816150371326821\n",
      "In Epoch 4/15, Train Loss: 0.8185751529644801\n",
      "In Epoch 5/15, Train Loss: 0.6444968240919398\n",
      "In Epoch 6/15, Train Loss: 0.5514573614988754\n",
      "In Epoch 7/15, Train Loss: 0.49430621426496935\n",
      "In Epoch 8/15, Train Loss: 0.45596171013200715\n",
      "In Epoch 9/15, Train Loss: 0.42847017921618563\n",
      "In Epoch 10/15, Train Loss: 0.4076928925285461\n",
      "In Epoch 11/15, Train Loss: 0.39111244686440366\n",
      "In Epoch 12/15, Train Loss: 0.37802284847952916\n",
      "In Epoch 13/15, Train Loss: 0.36692707617082065\n",
      "In Epoch 14/15, Train Loss: 0.3573707253344532\n",
      "In Epoch 15/15, Train Loss: 0.3492810119634498\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(train_loader, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X,y in train_loader: #loop through the batches created and train one batch at a time (enumerate a loader object return a tuple: (batch number, training data)\n",
    "            #forward pass\n",
    "            predictions = model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            #backpropagation\n",
    "            loss.backward() #figure derivatives with respect to each parameter\n",
    "            optimizer.step() #adjust the parameters using gradient descent\n",
    "            optimizer.zero_grad() #clear out the derivative values computed in backward command\n",
    "            epoch_loss += loss.item()\n",
    "        print(\"In Epoch \" + str(i+1) + \"/\" + str(num_epochs) + \", Train Loss: \" + str(epoch_loss/len(train_loader))) #report average loss over all the batches     \n",
    "\n",
    "num_epochs =  15 # feel free to change the number of training epochs\n",
    "train(train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52aa6c8-7893-4249-bdb9-644bdcb33ae5",
   "metadata": {},
   "source": [
    "##### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "253b7277-15be-47e4-9c9e-1638919d0550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.55 %\n"
     ]
    }
   ],
   "source": [
    "def evaluation(test_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): #don't need to store gradients this time\n",
    "        for X, y in test_loader:\n",
    "            predictions = model(X)\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    test_accuracy = 100 * (correct / total)\n",
    "    print('Test Accuracy: %.2f %%' % test_accuracy)\n",
    "            \n",
    "            \n",
    "evaluation(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2e49d-ffac-4beb-a814-b530ea9d3f52",
   "metadata": {},
   "source": [
    "###### Model Review\n",
    "Since we have balanced classes and don't have any particular cost or risk difference between false positives and false negatives, accuracy is an appropriate metric to use in this case. With 10 balanced classes, our random baseline would be 10%. Our accuracy of 90% far exceeds our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb647ca-96bf-4085-8f22-62e37cee3319",
   "metadata": {},
   "source": [
    "#### Model 2: Brain Tumor Binary Classification - Unbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b554b7-c2de-4f94-843a-f8b4c1cd865e",
   "metadata": {},
   "source": [
    "##### Data Import and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdc6fa6-273e-4bc5-953b-33b4a23d4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/trina/.cache/huggingface/datasets/sartajbhuvaji___imagefolder/sartajbhuvaji--Brain-Tumor-Classification-5ca63585a73c92fc/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f875425a4247a7ba0415f596183b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import brain tumor classification data from hugging face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sartajbhuvaji/Brain-Tumor-Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a10a810-0e2d-4243-9f62-c5e2ac47a826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 3264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb1d4390-cc49-4483-9255-68864e0c5ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  label\n",
       "0     <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n",
       "1     <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n",
       "2     <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n",
       "3     <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n",
       "4     <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n",
       "...                                                 ...    ...\n",
       "3259  <PIL.JpegImagePlugin.JpegImageFile image mode=...      3\n",
       "3260  <PIL.JpegImagePlugin.JpegImageFile image mode=...      3\n",
       "3261  <PIL.JpegImagePlugin.JpegImageFile image mode=...      3\n",
       "3262  <PIL.JpegImagePlugin.JpegImageFile image mode=...      3\n",
       "3263  <PIL.JpegImagePlugin.JpegImageFile image mode=...      3\n",
       "\n",
       "[3264 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e2e1461-5714-4087-bbf1-e5e8ee016179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want this to be a binary classification - original labels are multi-class: 0, 1, 3 are types of tumors, 2 is no tumor\n",
    "df['label'] = [1 if lbl == 2 else 0 for lbl in df['label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d35099b2-635b-49b7-abb9-fff9fa787fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f705b93-a7f0-4d9d-bd38-e36b2165b8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8468137254901961"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df['label'].value_counts()\n",
    "baseline = classes[0] / classes.sum()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6eea88-547c-4378-bfdd-7d923f6cc5a2",
   "metadata": {},
   "source": [
    "Classes are imbalanced. This will affect the metric we choose to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65d79f72-8057-41e6-a3ff-9eda11994616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels to tensor\n",
    "\n",
    "# Extract the values from the column\n",
    "labels = df['label'].values\n",
    "\n",
    "# Convert the values to a PyTorch tensor\n",
    "labels_tensor = torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e80ed2a4-1636-4f87-afdc-15a7dc303521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff65f8e8-068f-4907-bd3a-64027b4b5bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 581, 524])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO: delete - for learning purposes\n",
    "#torch and normalize and split\n",
    "transform = transforms.ToTensor()\n",
    "transform(df['image'].iloc[3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24e06c50-82fb-47de-9b83-526980e07b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3264, 3, 400, 380])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert images to a tensor\n",
    "#My images are of varying size so need to resize, convert to a tensor and normalize\n",
    "desired_height = 400\n",
    "desired_width = 380\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((desired_height, desired_width)),  # Resize the images to a common size\n",
    "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor values\n",
    "])\n",
    "images_tensor = torch.stack([transform(img) for img in df['image']])#transform each image then put all images into one tensor\n",
    "images_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a920f72a-156b-4a38-93aa-84d74389f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test data sets and create dataLoader objects\n",
    "batch_size = 64\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "# Create DataLoader objects for training and test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19cd6420-3051-4a8e-bc11-64978054e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model (use MLP defined above)\n",
    "\n",
    "input_size = 3 * desired_height * desired_width\n",
    "hidden_size = 528 \n",
    "output_size = 2  # binary classification\n",
    "binary_model = MLP(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d825984-7ed9-4764-ae99-6b668d7bb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "learning_rate =  .001 \n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD (binary_model.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b237ac3d-3eab-4b80-aa4f-9604ab98b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Epoch 1/15, Train Loss: 0.64912066808561\n",
      "In Epoch 2/15, Train Loss: 0.6490452871090029\n",
      "In Epoch 3/15, Train Loss: 0.6490434306423839\n",
      "In Epoch 4/15, Train Loss: 0.6490060759753715\n",
      "In Epoch 5/15, Train Loss: 0.649039444400043\n",
      "In Epoch 6/15, Train Loss: 0.6490765170353215\n",
      "In Epoch 7/15, Train Loss: 0.6490788270787495\n",
      "In Epoch 8/15, Train Loss: 0.6490993179926058\n",
      "In Epoch 9/15, Train Loss: 0.6490310881195999\n",
      "In Epoch 10/15, Train Loss: 0.6490929533795613\n",
      "In Epoch 11/15, Train Loss: 0.6490422181966828\n",
      "In Epoch 12/15, Train Loss: 0.6490418780140761\n",
      "In Epoch 13/15, Train Loss: 0.6490640654796507\n",
      "In Epoch 14/15, Train Loss: 0.6490285861782912\n",
      "In Epoch 15/15, Train Loss: 0.6490597913904887\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(train_loader, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X,y in train_loader: #loop through the batches created and train one batch at a time (enumerate a loader object return a tuple: (batch number, training data)\n",
    "            #forward pass\n",
    "            predictions = binary_model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            #backpropagation\n",
    "            loss.backward() #figure derivatives with respect to each parameter\n",
    "            optimizer.step() #adjust the parameters using gradient descent\n",
    "            optimizer.zero_grad() #clear out the derivative values computed in backward command\n",
    "            epoch_loss += loss.item()\n",
    "        print(\"In Epoch \" + str(i+1) + \"/\" + str(num_epochs) + \", Train Loss: \" + str(epoch_loss/len(train_loader))) #report average loss over all the batches     \n",
    "\n",
    "num_epochs =  15 \n",
    "train(train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1656ea53-b33c-4f39-b458-a2094e71fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.99 %\n",
      "Test Precision: 25.00 %\n",
      "Test Recall: 2.13 %\n",
      "Test F1: 3.92 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluation(test_loader):\n",
    "    predictions_list = []\n",
    "    true_labels_list = []\n",
    "    \n",
    "    with torch.no_grad(): #don't need to store gradients this time\n",
    "        for X, y in test_loader:\n",
    "            predictions = binary_model(X)\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            predictions_list.extend(predicted.tolist())\n",
    "            true_labels_list.extend(y.tolist())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracy = accuracy_score(true_labels_list, predictions_list) * 100\n",
    "    test_precision = precision_score(true_labels_list, predictions_list, average='binary') * 100\n",
    "    test_recall = recall_score(true_labels_list, predictions_list, average='binary') * 100\n",
    "    test_F1 = f1_score(true_labels_list, predictions_list, average='binary') * 100\n",
    "    \n",
    "    print('Test Accuracy: %.2f %%' % test_accuracy)\n",
    "    print('Test Precision: %.2f %%' % test_precision)\n",
    "    print('Test Recall: %.2f %%' % test_recall)\n",
    "    print('Test F1: %.2f %%' % test_F1)\n",
    "\n",
    "evaluation(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb1dfa-542d-4030-894c-151e20c964cd",
   "metadata": {},
   "source": [
    "This model is specifically trying to detect brain tumors based on MRI images. The classes are highly unbalanced (our baseline accuracy is 84.6 %). The fact that this model's accuracy is pretty much the same as our baseline not only means that this is very poor model but likely that the model mostly just learned to categorize all data as the majority class. Since there is a high cost to missing a brain tumor, we would prefer our model catch all positive (non-majority) cases (even if it falsely identifies some as positive). This means that recall would be a meaningful evaluation metric for us. A recall of 2% indicates that this model only identifies 2% of the actual positives as positive. This would be disastrous and would rule out this model (at least without serious modification). There is also some justification to look at the precision metric. Receiving a false positive would lead to additional expensive testing as well as a significate amount of stress to the patients. While I believe recall is more pressing (making sure we don't miss any positives), there is still a cost associated with false positives. A precision of 25% indicates that when we predict positive, we are correct 25% of the time. This again shows this to be a poor model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a44205-8cf9-40fe-ba08-608e27ee22c1",
   "metadata": {},
   "source": [
    "#### Model 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadf380b-23e1-4bb4-975c-5a37d3bf55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create (or import) model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5004c-fff4-420c-9e73-3f0b0080d8ae",
   "metadata": {},
   "source": [
    "### 2.3 Discuss the potential shortcomings of the metrics that you choose. What’s the possible solutions to improve the shortcomings? (20 points)\n",
    "[TODO: Discuss here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaec6a-4d26-4caf-8584-eaa09284f445",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- \"Accuracy vs. precision vs. recall in machine learning: what's the difference?\" https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall#:~:text=Accuracy%20is%20a%20metric%20that,often%20the%20model%20is%20right%3F\n",
    "- \"Understanding and Applying F1 Score: AI Evaluation Essentials with Hands-On Coding Example\" https://arize.com/blog-course/f1-score/#:~:text=F1%20score%20is%20often%20preferred,number%20of%20non%2Dspam%20emails.\n",
    "- \"How to explain the ROC curve and ROC AUC score?\" https://www.evidentlyai.com/classification-metrics/explain-roc-curve\n",
    "- \"What Percentage of Abnormal Mammograms Are Cancer?\" https://www.medicinenet.com/what_percentage_of_abnormal_mammograms_are_cancer/article.htm\n",
    "- Dataset: \"sartajbhuvaji/Brain-Tumor-Classification\" https://huggingface.co/datasets/sartajbhuvaji/Brain-Tumor-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a9943-c809-4d6c-9e87-e57da8b11c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
