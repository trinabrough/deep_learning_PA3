{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c421744c-40d9-4825-a546-40450e6663c5",
   "metadata": {},
   "source": [
    "# PA 3: Evaluation and Comparison on Deep learning Models\n",
    "Trina Brough Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c04ac3-264d-4989-8b77-0bfec1eaf275",
   "metadata": {},
   "source": [
    "## 1. Understanding Evaluation Metrics (20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec6b1d-e772-44c6-a94d-6305be1f7bb7",
   "metadata": {},
   "source": [
    "### 1.1 Please explain the following commonly used evaluation metrics. (10 points)\n",
    "- Accuracy: Accuracy is how often the model predicts the outcome correctly. It is the percentage correct calculated by: the number of samples correctly classified divided by the total number of samples. It answers the question: how often is the model right? Accuracy is simple to calculate and easy to understand but it is not very helpful if our classes are imbalanced or if what we really care about is predicting events that rarely occur.\n",
    "- Precision: Precision measures how often a model correctly predicts the positive class out of the predicted positives. It is the percentage calculated by: the number of true positive predictions divided by the total number of positive predictions (both true and false). It answers the question: how often are the positive predictions correct? Precision addresses the problem that accuracy has when dealing with unbalanced classes. It is especially helpful when the cost of a false positive is high (but are okay if you miss some positives (false negatives)). Precision is best when we care more about \"being right\" than detecting them all.\n",
    "- Recall: Recall measures how often a model correctly predicts the positive (true positives) out of all the actual positives. It is the percentage calculated by: the number of true positive predictions divided by the total number of positives (true positives plus false negatives). It answers the question: how well does a model find all instances of the positive class? Recall addressed the problem that accuracy has when dealing with unbalanced classes. It is especially helpful when the cost of a false negative is high (but are okay with some false positives). Recall is best when we care more about detecting them all than \"being right\".\n",
    "- F1 Score: The F1 score is a way of averaging the precision and recall rates into one value. Because precision and recall are both rates, it creates this using the harmonic mean: 2 * (Precision * Recall) / (Precision + Recall). The result is a number between 0 and 1 that indicates how well a model classifies samples into their correct classes (0 being not classifying anything correctly and 1 being classifying all samples correctly). F1 scores is used to evaluate LLM accuracy as well as binary and multi-class classification problems (especially when classes are unbalanced). It is useful when wanting to account for both precision and recall and the costs of false negatives and false positives are relatively even. If one is more costly than the other, using straight precision or recall would be best.\n",
    "- ROC Curve and AUC (Area Under the Curve): ROC is a graph that plots the True Positive Rate (Recall) on the y axis against the False Positive Rate (1 -  Precision, can be thought of as the \"False Alarm Rate\") on the x axis. The multiple curves are created for the classification model using different thresholds. These can then be compared on the ROC curve. The random baseline will be a straight line from the bottom left of the graph to the top right of the graph. The more the ROC curves upwards away from the baseline, the better the model is. (A perfect ROC curve would actually run staight up the y axis until 1, then turn right and run straight vertically). This upward curve is quantified by the AUC metric. It measures the area under the curve and allows us to determined which threshold is best (the largest AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6075e0f-70e0-4a9c-acff-073651f1b996",
   "metadata": {},
   "source": [
    "### 1.2 Provide a practical scenario and explain which metric(s) should be chosen to assess the model performance in that scenario. (10 points)\n",
    "Scenario: A binary classification model to identify tumors in mammogram images. The data used for this model would have unbalanced classes, since there are many more mammograms taken of healthy breast tissue than of tumors. (Of all women getting mammograms, only 0.5% have breast cancer). This means that accuracy would not be a good metric (we could classify as tumor-free every time and would be correct 99.5% of the time). We could choose to use the F1 score if the cost of false negatives and false positives is similar. However, in this case, a false positive means a woman would have the stress of coming in for more diagnostic testing but a false negative would prevent her from receiving possible life-saving treatment. The cost of a false negative is much higher than a false positive. In this case recall would be our most important metric. We want to make sure we detect all the true positives even if we end up with some false positives because lives are at stake. This isn't true without bounds, however. We could, in theory, classify every image as positive and we'd have 100% recall. This is the same problem as accuracy on the flip-side. This would result in a large cost (and stress) burden as women are re-tested unnecessarily. We might, therefore, want to plot this model using different thresholds on an ROC curve. We want to maximize our recall, which would induce us to perhaps over-categorize positives (perhaps adjusting our threshold more towards a positive classification). The ROC curves would allow us to compare the different thresholds while balancing our recall against the false positive rate. The AUC would quantify which threshold is best (in case it's not visually clear on the graph).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df697539-65f3-4e67-8148-cccba36bf51c",
   "metadata": {},
   "source": [
    "## 2. Model Comparison (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920e5ea-be4b-47ba-8803-43d5fc01cc49",
   "metadata": {},
   "source": [
    "### 2.1 Choose THREE commonly used deep-learning models and train these models for a specific task (e.g., image/text classification). (20 points)\n",
    "\n",
    "To gain understanding of the metrics described above, I'd like to choose three image classification models that would likely use different metrics for evaluations. To that end, I will use:\n",
    "-the MNIST as a multi-class image classification task with balanced classes. I will use the model we developed for PA 1.\n",
    "-\n",
    "\n",
    "[TODO: Describe what models/which tasks do you want to choose. Explain the motivation.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef786-3b83-46c6-8899-62bf46ce18b4",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate these models using appropriate metrics, and analyze the results. Complete this comparison using Jupyter Notebook. (40 points)\n",
    "[TODO: Present the evaluation results with appropriate figures and tables here.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851ff360-5b74-4f54-bbc7-ef295c981d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef783cc-d897-4c85-92be-fe2fa0437c53",
   "metadata": {},
   "source": [
    "#### Model 1: MNIST Multi-Class Classification - Balanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f40de-ea1a-4ddf-8c83-0c583553ce54",
   "metadata": {},
   "source": [
    "##### Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f5c29-53b9-4f7b-8482-b25394fe4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "# ref: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# train data\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test data\n",
    "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae61abc-88a5-410c-8797-bb95aedf1697",
   "metadata": {},
   "source": [
    "##### Build MLP model for MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256b796-154e-4eea-9ed5-b381b3070dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your Multi-Layer Perceptron (MLP) model\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: build layers here\n",
    "        self.linear_RELU_stack = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(input_size, hidden_size), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: build the networks\n",
    "        x = self.linear_RELU_stack(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071498e-3c13-4361-8718-57161ba32749",
   "metadata": {},
   "source": [
    "##### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b09ebd-2f9e-4801-8001-28380f7be44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLP model\n",
    "\n",
    "input_size = 28 * 28  # what's input size? hints: flattened size for one instance\n",
    "hidden_size = 528  # define on your own\n",
    "output_size = 10  # what's output size for this classification tasks?\n",
    "model = MLP(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b22b13-cebd-469f-9102-63fff02a8cdc",
   "metadata": {},
   "source": [
    "##### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce78f29-e9c2-4239-9168-ea46467ad57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "learning_rate =  .001 # define your own learning rate here\n",
    "criterion = torch.nn.CrossEntropyLoss()  # which loss function should we use here? Using Cross Entropy, online this is mentioned as a good loss function for MNIST categorization\n",
    "optimizer = torch.optim.SGD (model.parameters(), lr = learning_rate)  # which optimizer do you use? Using Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d543-137e-42fa-83d1-71c2a6a114c3",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4dfa5c-5d26-4e84-85b7-d31ac22cc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(train_loader, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X,y in train_loader: #loop through the batches created and train one batch at a time (enumerate a loader object return a tuple: (batch number, training data)\n",
    "            #forward pass\n",
    "            predictions = model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            #backpropagation\n",
    "            loss.backward() #figure derivatives with respect to each parameter\n",
    "            optimizer.step() #adjust the parameters using gradient descent\n",
    "            optimizer.zero_grad() #clear out the derivative values computed in backward command\n",
    "            epoch_loss += loss.item()\n",
    "        print(\"In Epoch \" + str(i+1) + \"/\" + str(num_epochs) + \", Train Loss: \" + str(epoch_loss/len(train_loader))) #report average loss over all the batches     \n",
    "\n",
    "num_epochs =  15 # feel free to change the number of training epochs\n",
    "train(train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52aa6c8-7893-4249-bdb9-644bdcb33ae5",
   "metadata": {},
   "source": [
    "##### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b7277-15be-47e4-9c9e-1638919d0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): #don't need to store gradients this time\n",
    "        for X, y in test_loader:\n",
    "            predictions = model(X)\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    test_accuracy = 100 * (correct / total)\n",
    "    print('Test Accuracy: %.2f %%' % test_accuracy)\n",
    "            \n",
    "            \n",
    "evaluation(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2e49d-ffac-4beb-a814-b530ea9d3f52",
   "metadata": {},
   "source": [
    "##### Model Review\n",
    "Since we have balanced classes and don't have any particular cost or risk difference between false positives and false negatives, accuracy is an appropriate metric to use in this case. With 10 balanced classes, our random baseline would be 10%. Our accuracy of #TODO:fillIn far exceeds our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb647ca-96bf-4085-8f22-62e37cee3319",
   "metadata": {},
   "source": [
    "#### Model 2: Brain Tumor Binary Classification - Unbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdc6fa6-273e-4bc5-953b-33b4a23d4e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8c04d24b3841d19f5af5bf4c121677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/310 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/sartajbhuvaji--Brain-Tumor-Classification to C:/Users/trina/.cache/huggingface/datasets/sartajbhuvaji___imagefolder/sartajbhuvaji--Brain-Tumor-Classification-5ca63585a73c92fc/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909d662af1724000b57c58d3b5c16659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d7d07020e34a5bbe7963efd4034e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c054a98fa6a849518d2fe02a25a2b93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117ebf7754784110b770be46bd1981e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/78.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df70da656ceb4279b49dcdf48a0960cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to C:/Users/trina/.cache/huggingface/datasets/sartajbhuvaji___imagefolder/sartajbhuvaji--Brain-Tumor-Classification-5ca63585a73c92fc/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a28809b679145eca9e20a8fb6f282b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import brain tumor classification data from hugging face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sartajbhuvaji/Brain-Tumor-Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a10a810-0e2d-4243-9f62-c5e2ac47a826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 3264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a44205-8cf9-40fe-ba08-608e27ee22c1",
   "metadata": {},
   "source": [
    "#### Model 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadf380b-23e1-4bb4-975c-5a37d3bf55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create (or import) model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5004c-fff4-420c-9e73-3f0b0080d8ae",
   "metadata": {},
   "source": [
    "### 2.3 Discuss the potential shortcomings of the metrics that you choose. Whatâ€™s the possible solutions to improve the shortcomings? (20 points)\n",
    "[TODO: Discuss here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaec6a-4d26-4caf-8584-eaa09284f445",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- \"Accuracy vs. precision vs. recall in machine learning: what's the difference?\" https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall#:~:text=Accuracy%20is%20a%20metric%20that,often%20the%20model%20is%20right%3F\n",
    "- \"Understanding and Applying F1 Score: AI Evaluation Essentials with Hands-On Coding Example\" https://arize.com/blog-course/f1-score/#:~:text=F1%20score%20is%20often%20preferred,number%20of%20non%2Dspam%20emails.\n",
    "- \"How to explain the ROC curve and ROC AUC score?\" https://www.evidentlyai.com/classification-metrics/explain-roc-curve\n",
    "- \"What Percentage of Abnormal Mammograms Are Cancer?\" https://www.medicinenet.com/what_percentage_of_abnormal_mammograms_are_cancer/article.htm\n",
    "- Dataset: \"sartajbhuvaji/Brain-Tumor-Classification\" https://huggingface.co/datasets/sartajbhuvaji/Brain-Tumor-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a9943-c809-4d6c-9e87-e57da8b11c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
